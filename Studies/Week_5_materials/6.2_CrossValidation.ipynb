{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a technique used in machine learning to assess how well a model will generalize to an independent data set. It is primarily used to estimate the effectiveness of a predictive model on unseen data. Here's a simple explanation:\n",
    "\n",
    "Imagine you have a dataset that you want to use to build a machine learning model. To ensure that your model doesn't just memorize the data (a problem known as overfitting), you need to test it on data it hasn't seen during training. However, you might not always have a separate test dataset. This is where cross-validation comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Split the Data: You divide your dataset into smaller parts. A common method is k-fold cross-validation, where 'k' is a number like 5 or 10. In k-fold cross-validation, you split your data into 'k' equal parts.\n",
    "\n",
    "2. Train and Test in Rounds: For each round, you use one of the 'k' parts as a test set and the rest as a training set. You train your model on the training set and then test it on the test set.\n",
    "\n",
    "3. Rotate and Repeat: You repeat this process 'k' times, each time using a different part as the test set. This way, each part of your data gets to be used as a test set exactly once.\n",
    "\n",
    "4. Average the Results: After you've trained and tested your model 'k' times, you average the performance (like accuracy) from each round. This gives you a good idea of how well your model is likely to perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Choose the number of folds\n",
    "k = 5\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize a model (using Logistic Regression here as an example)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Store the accuracy for each fold\n",
    "accuracies = []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # print(train_index, test_index)\n",
    "    # Split the data into training and test sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions and evaluate the model\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "    # Store the accuracy\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate the average accuracy across all folds\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average Accuracy: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Score: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load a sample dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Initialize a model (using Logistic Regression here as an example)\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Choose the number of folds\n",
    "k = 5\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "# This function returns the scores for each fold automatically\n",
    "scores = cross_val_score(model, X, y, cv=k)\n",
    "\n",
    "# Calculate the average score across all folds\n",
    "average_score = scores.mean()\n",
    "\n",
    "print(f\"Average Score: {average_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Macro Recall: 0.9733333333333334\n",
      "Average Weighted Recall: 0.9733333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nMacro Recall:\\n\\nDefinition: Macro recall is calculated by computing the recall for each class independently and then taking the average of these recalls. This method treats all classes equally, regardless of their frequency in the dataset.\\nCalculation: If you have N classes, calculate the recall for each class, and then average these values.\\nUse Case: Macro recall is useful when you want to treat all classes equally and when each class is equally important.\\n\\n\\nWeighted Recall:\\n\\nDefinition: Weighted recall, on the other hand, calculates recall for each class like macro recall but then takes a weighted average of these scores. The weight for each class's recall is proportional to the number of true instances of that class in the dataset.\\nCalculation: Compute the recall for each class, then average these values, weighting each one by the proportion of true instances of that class in the dataset.\\nUse Case: Weighted recall is useful when class imbalance is present in the dataset. It gives more weight to the majority class and is more representative of the model's performance across the most common classes.\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Measuring recall from cross validation\n",
    "\n",
    "# Load a multiclass classification dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Perform 5-fold cross-validation measuring macro or weighted recall\n",
    "scores_macro = cross_val_score(model, X, y, cv=5, scoring='recall_macro')\n",
    "scores_weighted = cross_val_score(model, X, y, cv=5, scoring='recall_weighted')\n",
    "\n",
    "# Calculate the average recall\n",
    "average_recall_macro = scores_macro.mean()\n",
    "average_recall_weighted = scores_weighted.mean()\n",
    "\n",
    "print(f\"Average Macro Recall: {average_recall_macro}\")\n",
    "print(f\"Average Weighted Recall: {average_recall_weighted}\")\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Macro Recall:\n",
    "\n",
    "Definition: Macro recall is calculated by computing the recall for each class independently and then taking the average of these recalls. This method treats all classes equally, regardless of their frequency in the dataset.\n",
    "Calculation: If you have N classes, calculate the recall for each class, and then average these values.\n",
    "Use Case: Macro recall is useful when you want to treat all classes equally and when each class is equally important.\n",
    "\n",
    "\n",
    "Weighted Recall:\n",
    "\n",
    "Definition: Weighted recall, on the other hand, calculates recall for each class like macro recall but then takes a weighted average of these scores. The weight for each class's recall is proportional to the number of true instances of that class in the dataset.\n",
    "Calculation: Compute the recall for each class, then average these values, weighting each one by the proportion of true instances of that class in the dataset.\n",
    "Use Case: Weighted recall is useful when class imbalance is present in the dataset. It gives more weight to the majority class and is more representative of the model's performance across the most common classes.\n",
    "\n",
    "''' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
