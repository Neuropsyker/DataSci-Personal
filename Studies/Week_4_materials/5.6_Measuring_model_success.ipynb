{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring the success of a model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recall\n",
    "\n",
    "- Measure of how well a model identifies all relevant instances of a particular class\n",
    "- Recall is seen as the percentage of actual positive cases that the model correctly predicted as positive\n",
    "\n",
    "$${Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "- TP (True Positives) are the instances correctly identified as positive by the model\n",
    "- FN (False Negatives) are the instances that are positive but incorrectly identified as negative by the model \n",
    "\n",
    "##### Precision\n",
    "\n",
    "- Measure of the accuracy og the positive predictions made by the model. \n",
    "- It tells you how many of the instances that the model predicted as positives are actually positives\n",
    "\n",
    "$${Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "- TP is the number of positive instances correctly identified by the model \n",
    "- FP is the number of negative instances that the model incorrectly identified as positive\n",
    "- A high precision means that when the model predicts a positive results it is more likely to be correct\n",
    "\n",
    "##### F1 score\n",
    "\n",
    "- The F1 score is used to measure a models performance when the classes are imbalanced. \n",
    "\n",
    "$$ F1 Score = 2* \\frac{Precision * Recall} {Precision + Recall}$$\n",
    "\n",
    "- The F1 score ranges from 0 to 1, where 1 is the best possible score and 0 is the worst \n",
    "- A high F1 score indicates that the model has a good balance of precision and recall \n",
    "- The metric is especially useful when you need a single measure to compare models\n",
    "\n",
    "##### Confusion matrix\n",
    "- Used in machine learning to visualise the performance of a classification algorithm \n",
    "- It's useful for understnading how well a model is performing in classifying different categories\n",
    "- TP: Number of positive instances correctly classified \n",
    "- FN: Number of negative instances incorrectly classified\n",
    "- FP: Number of negative instances incorrectly classified as positive\n",
    "- TN: Number of negative instances correctly classified as negative \n",
    "\n",
    "<br>\n",
    "\n",
    "Summary view\n",
    "\n",
    "|                     | **Predicted Positive** | **Predicted Negative** |\n",
    "|---------------------|------------------------|------------------------|\n",
    "| **Actual Positive** | True Positive (TP)     | False Negative (FN)    |\n",
    "| **Actual Negative** | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "With numbers\n",
    "|                     | **Predicted Positive** | **Predicted Negative** |\n",
    "|---------------------|------------------------|------------------------|\n",
    "| **Actual Positive** | TP: 50 (50%)           | FN: 10 (10%)           |\n",
    "| **Actual Negative** | FP: 5 (5%)             | TN: 35 (35%)           |\n",
    "\n",
    "\n",
    "##### AUC\n",
    "\n",
    "The area under the curve, refers to the area under the receiver operating characteristic curve (ROC). \n",
    "\n",
    "1. TPR\n",
    "- The ROC is a graphical plot that illustrates the diagnostic ability of a binary classifier system. \n",
    "- It creates the TPR - true positive rate\n",
    "- It measures how many actual positives are correctly identified \n",
    "\n",
    "$$ TPR = \\frac{TP}{TP+FN} $$ \n",
    "\n",
    "2. FPR \n",
    "- It measures how many actual positives are correctly identified \n",
    "\n",
    "$$ FPR = \\frac{FP}{FP+TN} $$\n",
    "\n",
    "3. AUC \n",
    "- Measures the entire 2D areas underneath the entire ROC curve \n",
    "- Provides an aggregate measure of performance across all possible classification thresholds\n",
    "- AUC ranges from 0-to-1 \n",
    "- AUC is used as a summary of the model's ability to distingush between positive and negative classes, the higher the AUC the better the model is at predicting the right classes \n",
    "- It measures the quality of the model predictions irrespective of what classification threshold is chosen \n",
    "- AUC is good for evaluating classifiers on imbalenced datasets, where accuracy can be misleading\n",
    "- A model that perfectly seperates the positive and negative classes will have an AUC of 1.0. \n",
    "- A model with an AUC less than 0.5 is performing worse than random guessing \n",
    "- A model with an AUC of about 0.5, is essentially making random guesses and it has an equal chance of classigying a positive or negative sample correctly \n",
    "\n",
    "\n",
    "\n",
    "##### ROC_AUC\n",
    "\n",
    "- It is used as a summary of the model's ability to distinguish between the postive and negative classes, the higher the AUC the better the model is at predicting 0's as 0's and 1s as 1's\n",
    "- It measures the quality of the model's predictions irrespective of what the classification threshold \n",
    "- AUC evaluates a model's performance across all possible thresholds, wheras accuracy evaluates performance at a specific threshold \n",
    "\n",
    "\n",
    "##### Log loss\n",
    "\n",
    "- A performance metric used in classification models \n",
    "- Specifically binary models \n",
    "- It measures the uncertainty of the probabilities predicted by a model with respect to the true labels \n",
    "\n",
    "$$ \\text {Log Loss} = -\\frac{1}{N}[y_ilog(p_i) + (1-y_i)log(1-p_i)] $$\n",
    "- Log loss penalises both types of erroes but specifically those that are confident and wrong \n",
    "- A perfect model would have a log loss of 0\n",
    "- Log loss takes into account the uncertainty of the predictions, this means it uses the predicted probabilities\n",
    "- A model that is more confident in it's wrong predictons will be penalised more heavily \n",
    "- Log loss is particularly useful when you need a measure of how far off the predictions are in terms of the probability, not just in terms of the final classification.\n",
    "- It is widely used in proabilistic models, where understanding the uncertainty of predictions is important.\n",
    "\n",
    "##### Accuracy\n",
    "\n",
    "The formula for accuracy is:\n",
    "\n",
    "$$ Accuracy = \\frac{\\text {Number of Correct Predictions}}{\\text {Total number of predictions}} $$\n",
    "\n",
    "- Accuracy ranges from 0 to 1, where 1 indicates a perfect accuracy and 0 indicates a complete inaccuracy \n",
    "- An accuracy of 0/8 means that 80% of the predictions made by the model are circa 80% accurate\n",
    "- Good for general purposes and when the classes are balanced and the costs of false positives and false negatives are roughly the same"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
