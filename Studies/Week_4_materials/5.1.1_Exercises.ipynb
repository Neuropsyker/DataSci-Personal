{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10:45 - 11:45 review last year notes and look through how to implement p value on the r^2 values from the housing dataset\n",
    "- 11:45 - 12:45 Review this morning notes \n",
    "- 12:45 - 13:45 Lunch\n",
    "- 13:45 - 15:00 Exercises a go (if you finish this, then start of ucertify)\n",
    "- 15:00: I will send through the solutions and the explanations on gradient descent, send you the logistic regression Austrialian dataset on the  \n",
    "- 15:00 - 16:30: EDA on the Australian  \n",
    "\n",
    "\n",
    "\n",
    "1. Explain what the fisher exact test is \n",
    "2. Understand the null hypthesis and p-values\n",
    "3. Implement Gradient Descent for Linear Regression:\n",
    "\n",
    "- start with y=mx+b.\n",
    "- Create a synthetic dataset (x, y) where you know the parameters (m, b).\n",
    "- How would you create a synthetic dataset? \n",
    "- Write a function to calculate the Mean Squared Error (MSE) as the loss function.\n",
    "- Implement gradient descent to minimize the MSE and find the best values for m and b.\n",
    "- Plot the regression line before and after applying gradient descent.\n",
    "\n",
    "\n",
    "1. Use numpy arange / linspace to create an array of x values (100 values ) \n",
    "2. Use y = mx+c for those variables add a distortion (use the np.random function here)\n",
    "3. Plot a scatter graph of x and y \n",
    "4. Create a MSE function\n",
    "5. Create a gradient function - here please think through what you are trying to optimise against, so trying to optimise for m and b\n",
    "6. Identify the optimal m, b. And have an output of the loss \n",
    "7. Using matplot lib build a graph with the optimal regression line\n",
    "\n",
    "4. Look into Batch Gradient Descent, Stochastic Gradient Descent (SGD) and Mini-batch Gradient Descent"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
