{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research different data engineering tools #\n",
    "\n",
    "Top 10 data engineering tools in use today:\n",
    "\n",
    "Apache Ecosystem (Hadoop, Spark, Kafka, Flink): A suite of open-source tools for distributed storage (Hadoop), large-scale data processing (Spark), distributed streaming (Kafka), and stream processing (Flink).\n",
    "\n",
    "Apache Airflow: An open-source platform for orchestrating complex data workflows.\n",
    "\n",
    "Apache NiFi: An integrated data logistics platform for automating the movement of data between systems.\n",
    "\n",
    "Amazon S3 (Simple Storage Service): A scalable object storage service often used for data lakes.\n",
    "\n",
    "Google BigQuery: A fully-managed, serverless data warehouse for analytics.\n",
    "\n",
    "Apache Beam: A unified stream and batch processing model for big data processing.\n",
    "\n",
    "Talend: An open-source data integration platform for designing and executing data workflows.\n",
    "\n",
    "Databricks: A unified analytics platform based on Apache Spark for big data processing and machine learning.\n",
    "\n",
    "Snowflake: A cloud-based data warehousing platform that allows for scalable and flexible data storage and analysis.\n",
    "\n",
    "Microsoft Azure Data Factory: A cloud-based data integration service for building, scheduling, and managing data pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Databases in commercial or scientific use today #\n",
    "\n",
    "MySQL: An open-source relational database management system (RDBMS) known for its ease of use and scalability.\n",
    "\n",
    "PostgreSQL: An open-source object-relational database system with a strong reputation for reliability and extensibility.\n",
    "\n",
    "Microsoft SQL Server: A relational database management system developed by Microsoft, widely used in enterprise environments.\n",
    "\n",
    "Oracle Database: A powerful and feature-rich relational database management system commonly used in enterprise applications.\n",
    "\n",
    "MongoDB: A NoSQL database that uses a document-oriented data model, making it suitable for handling large volumes of unstructured data.\n",
    "\n",
    "Cassandra: A highly scalable NoSQL database designed to handle large amounts of distributed data across commodity servers.\n",
    "\n",
    "Redis: An in-memory data structure store used as a database, cache, and message broker, known for its speed and simplicity.\n",
    "\n",
    "SQLite: A self-contained, serverless, and zero-configuration relational database engine often used in embedded systems.\n",
    "\n",
    "Amazon DynamoDB: A fully managed NoSQL database service provided by Amazon Web Services (AWS), suitable for high-performance applications.\n",
    "\n",
    "Neo4j: A graph database that uses a flexible graph model for representing and querying complex relationships in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is ETL and how does it function? #\n",
    "\n",
    "\n",
    "ETL stands for Extract, Transform, Load, and it refers to a process widely used in data integration and warehousing. \n",
    "The primary goal of ETL is to move and transform data from source systems to a target system, such as a data warehouse, for analysis and reporting. \n",
    "\n",
    "The ETL process involves three main stages: extract, transform, and load.\n",
    "\n",
    "Extraction: data is collected from various source systems, which can include databases, applications, logs, and other data repositories. The data extraction may involve selecting specific subsets of information or pulling entire datasets. This phase ensures that relevant data is obtained from diverse sources for further processing.\n",
    "\n",
    "Transformation: the extracted data is cleaned, enriched, and restructured to meet the requirements of the target system or data warehouse. This involves data cleaning, normalization, aggregation, and the application of business rules. The transformation phase ensures that the data is consistent, accurate, and formatted appropriately for analysis and reporting.\n",
    "\n",
    "Loading: the transformed data is loaded into the target system, often a data warehouse or a database designed for analytical purposes. The loading phase can involve various strategies, such as incremental loading for efficiency, and it ensures that the transformed data is accessible for querying and reporting. The loaded data can be used for business intelligence, analytics, and decision-making processes.\n",
    "\n",
    "ETL processes are fundamental in managing the flow of data within an organization, enabling businesses to consolidate, clean, and analyze data from disparate sources to gain insights and make informed decisions. The ETL framework is a critical component of modern data architectures, supporting the integration and accessibility of data for business intelligence and analytics purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research the different types of SQL, what is the general syntax for SQL queries? #\n",
    "\n",
    "SQL (Structured Query Language) is a standard language for managing and manipulating relational databases. There are several types each serving a specific purpose:\n",
    "\n",
    "Data Query Language (DQL) is used for querying information from the database. \n",
    "The primary statement is SELECT, which retrieves data from one or more tables based on specified criteria.\n",
    "\n",
    "Data Definition Language (DDL) is used to define and manage the structure of a database. \n",
    "Key statements include CREATE (for creating database objects like tables and indexes), ALTER (for modifying database objects), and DROP (for deleting database objects).\n",
    "\n",
    "Data Manipulation Language (DML) statements are used to manipulate data stored in the database. \n",
    "The primary DML statement is INSERT (for adding new records), UPDATE (for modifying existing records), and DELETE (for removing records).\n",
    "\n",
    "Data Control Language (DCL) statements control access to data within the database. \n",
    "Key statements include GRANT (for granting specific permissions to users or roles) and REVOKE (for revoking permissions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the different cloud providers, and what is cloud computing? #\n",
    "\n",
    "Cloud computing refers to delivering computing services over the internet. \n",
    "These services include computing power, storage, databases, networking, analytics, and software. \n",
    "\n",
    "\n",
    "There are several major cloud service providers, each offering a range of services within their cloud platforms:\n",
    "\n",
    "Amazon Web Services (AWS): AWS is one of the largest and most widely used cloud platforms, offering a vast array of services, including computing power, storage, machine learning, analytics, and more.\n",
    "\n",
    "Microsoft Azure: Azure is Microsoft's cloud platform, providing services for computing, analytics, storage, and networking. It integrates well with Microsoft products and services.\n",
    "\n",
    "Google Cloud Platform (GCP): GCP offers cloud services for computing, data storage, machine learning, and big data analytics. Google's expertise in data management and analytics is a key feature of GCP.\n",
    "\n",
    "IBM Cloud: IBM Cloud provides a comprehensive suite of cloud services, including AI, blockchain, and Internet of Things (IoT). It also supports hybrid cloud deployments.\n",
    "\n",
    "Alibaba Cloud: Alibaba Cloud is a leading cloud provider in Asia, offering a wide range of services similar to other global providers. It is particularly popular in the Asia-Pacific region.\n",
    "\n",
    "Oracle Cloud: Oracle Cloud provides cloud services with a focus on database management, enterprise applications, and cloud infrastructure.\n",
    "\n",
    "Cloud computing can be further categorized into three main service models:\n",
    "\n",
    "Infrastructure as a Service (IaaS) provides virtualized computing resources over the internet. Users can rent virtual machines, storage, and networking.\n",
    "\n",
    "Platform as a Service (PaaS) provides a platform allowing customers to develop, run, and manage applications without dealing with the complexity of infrastructure.\n",
    "\n",
    "Software as a Service (SaaS) provides software applications over the internet on a subscription basis. E.g. users access applications through a web browser without needing to install or maintain software locally.\n",
    "\n",
    "\n",
    "Cloud computing brings advantages such as scalability, cost-effectiveness, flexibility, and accessibility: it provides a flexible and scalable way to access and utilize computing resources without the need for significant upfront expenditures in acquiring hardware and building infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the names of the data science cloud services that they provide? #\n",
    "\n",
    "The major cloud providers offer a variety of data science and machine learning services as part of their cloud platforms:\n",
    "\n",
    "Amazon Web Services (AWS):\n",
    "Amazon SageMaker: A fully managed service that enables developers and data scientists to build, train, and deploy machine learning models quickly.\n",
    "AWS Glue: A fully managed extract, transform, and load (ETL) service for preparing and loading data for analysis.\n",
    "\n",
    "Microsoft Azure:\n",
    "Azure Machine Learning: A cloud-based service for building, training, and deploying machine learning models at scale.\n",
    "Azure Databricks: An Apache Spark-based analytics platform for big data and machine learning.\n",
    "\n",
    "Google Cloud Platform (GCP):\n",
    "Google Cloud AI Platform: Provides tools to build, deploy, and manage machine learning models on GCP.\n",
    "BigQuery ML: A fully-managed machine learning service for building models directly within Google BigQuery.\n",
    "\n",
    "IBM Cloud:\n",
    "Watson Studio: An integrated environment for data scientists, developers, and business analysts to build, train, and deploy machine learning models.\n",
    "IBM Watson Machine Learning: Facilitates the deployment and management of machine learning models.\n",
    "\n",
    "Alibaba Cloud:\n",
    "Machine Learning Platform for AI: A comprehensive platform that provides a full range of AI and machine learning services.\n",
    "\n",
    "Oracle Cloud:\n",
    "Oracle Cloud Infrastructure Data Science: A fully managed data science platform that enables data scientists to collaborate and build, train, and deploy models.\n",
    "\n",
    "These services typically include capabilities for data exploration, model training, model deployment, and integration with other cloud services that you might need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the difference between ETL and ELT, why are there these differences? #\n",
    "\n",
    "ETL (Extract, Transform, Load):\n",
    "In the traditional ETL process, data is first extracted from source systems, then transformed, and finally loaded into the target system (usually a data warehouse). \n",
    "The transformation step takes place in an intermediary staging area or a dedicated ETL server before loading the data into the target system. ETL is well-suited for scenarios where data needs to be cleansed, aggregated, or otherwise transformed before being stored in the target database. \n",
    "ETL processes are often batch-oriented and scheduled at regular intervals.\n",
    "\n",
    "ELT (Extract, Load, Transform):\n",
    "In the ELT process, data is extracted from source systems and loaded directly into the target system without undergoing significant transformation. \n",
    "The transformation is then applied within the target system itself, often using the processing power of the target data warehouse. ELT leverages the computational capabilities of modern data warehouses to perform transformations on large datasets efficiently. \n",
    "This approach is well-suited for scenarios where the target system has robust processing capabilities and can handle the transformation workload effectively in realtime without needing scheduling.\n",
    "\n",
    "\n",
    "ETL is \"traditional\" but may be preferred when the target system lacks robust processing capabilities. ETL may also be suitable for complex transformations or scenarios with moderate data volumes.\n",
    "\n",
    "ELT leverages the processing power of modern data warehouses and can be advantageous when using platforms designed for large-scale data processing. ELT is well-suited for large-scale data volumes and scenarios where the target system can efficiently handle transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When would you use batch processing over streaming? #\n",
    "\n",
    "Volume of Data: Batch Processing is well-suited for scenarios with large volumes of historical or accumulated data that can be processed at once. \n",
    "If the data can be collected and processed periodically, batch processing is an efficient choice.\n",
    "\n",
    "Data Latency Tolerance: Batch Processing is suitable when low-latency processing is not critical, and there is tolerance for delays between data generation and processing. \n",
    "Batch processing typically involves periodic or scheduled runs, so results may not be immediately available.\n",
    "\n",
    "Complex Data Transformations: Batch Processing is ideal for scenarios requiring complex and resource-intensive data transformations. \n",
    "Since batch processing operates on entire datasets, it can efficiently perform extensive computations, aggregations, and analyses.\n",
    "\n",
    "Cost Considerations: Batch Processing can be cost-effective for large-scale processing when, for example, computational resources can be optimized and scheduled during off-peak times. \n",
    "Resources can be provisioned and scaled based on batch processing requirements.\n",
    "\n",
    "Simplicity of Development: Batch Processing is simpler to develop and manage for certain use cases. \n",
    "It is easier to reason about the state of the system when processing happens in well-defined batches, making it more straightforward to design, test, and troubleshoot.\n",
    "\n",
    "Data Consistency Requirements: Batch Processing is most suitable when data consistency across the entire dataset is critical. \n",
    "In batch processing, you have a snapshot of the entire dataset at a given point in time, ensuring consistency for analysis and reporting.\n",
    "\n",
    "Regulatory Compliance: Batch Processing can be preferred in industries or scenarios with specific regulatory requirements that align with periodic, auditable data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research other streaming tools other than Kafka #\n",
    "\n",
    "In addition to Apache Kafka, other popular streaming and event processing tools and frameworks that are widely used for building real-time data pipelines and processing streams of data include:\n",
    "\n",
    "Apache Flink: Apache Flink is a powerful and flexible stream processing framework for big data processing and analytics. \n",
    "It supports both event time and processing time semantics and provides a rich set of APIs for building complex stream processing applications.\n",
    "\n",
    "Apache Pulsar: Apache Pulsar is a distributed messaging and event streaming platform that provides a durable, scalable, and flexible solution for real-time event-driven applications. \n",
    "It supports publish-subscribe and queue semantics.\n",
    "\n",
    "Amazon Kinesis: Amazon Kinesis is a cloud-based platform by AWS that includes several services for real-time data streaming. Kinesis Data Streams, Kinesis Data Firehose, and Kinesis Data Analytics are components that enable streaming ingestion, storage, and analytics.\n",
    "\n",
    "Azure Stream Analytics: Azure Stream Analytics is a fully managed real-time analytics service by Microsoft Azure. \n",
    "It enables the processing and analyzing of streaming data from various sources, such as IoT devices, sensors, and application logs.\n",
    "\n",
    "Google Cloud Dataflow: Google Cloud Dataflow is a fully managed stream and batch processing service provided by Google Cloud. \n",
    "It allows users to build streaming and batch processing pipelines using Apache Beam, supporting both real-time and historical data processing.\n",
    "\n",
    "RabbitMQ: RabbitMQ is a popular open-source message broker that supports publish-subscribe and message queue patterns. \n",
    "While not strictly a stream processing tool, it is commonly used for building event-driven architectures.\n",
    "\n",
    "Storm: Apache Storm is a distributed stream processing system known for its low-latency and fault-tolerant processing capabilities. \n",
    "It is particularly suitable for real-time analytics and event processing.\n",
    "\n",
    "Samza: Apache Samza is a distributed stream processing framework that is part of the Apache Kafka project. \n",
    "It is designed to process event streams with low-latency and high-throughput.\n",
    "\n",
    "Redis Streams: Redis Streams is a feature within the Redis key-value store that allows for building simple, scalable, and real-time data streaming solutions.\n",
    "\n",
    "NATS Streaming: NATS Streaming is an event streaming platform built on top of the NATS messaging system. It provides features such as at-least-once delivery, message replay, and durability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research batch processing tools #\n",
    "\n",
    "Apache Hadoop Ecosystem:\n",
    "Components: Hadoop Distributed File System (HDFS), MapReduce.\n",
    "Description: Apache Hadoop is a foundational framework for distributed storage and batch processing. \n",
    "It includes HDFS for distributed storage and MapReduce for processing large-scale data in parallel.\n",
    "\n",
    "Apache Spark:\n",
    "Description: Apache Spark is a versatile, open-source data processing engine that supports both batch and real-time processing. \n",
    "It provides a unified analytics engine for big data processing.\n",
    "\n",
    "Apache Flink:\n",
    "Description: Apache Flink is a stream processing framework that also supports batch processing. \n",
    "It provides event-driven processing for real-time analytics and batch processing capabilities.\n",
    "\n",
    "Talend:\n",
    "Description: Talend is a data integration platform that provides tools for designing, testing, and executing batch processing jobs. \n",
    "It supports various data processing and transformation tasks.\n",
    "\n",
    "Luigi:\n",
    "Description: Luigi is an open-source Python framework for building complex data pipelines. \n",
    "It allows users to define tasks, dependencies, and workflows, making it easier to schedule and execute batch processing jobs.\n",
    "\n",
    "Apache Airflow:\n",
    "Description: Apache Airflow is an open-source platform for orchestrating complex workflows. \n",
    "While commonly used for managing workflows that involve batch processing tasks, it can also handle real-time tasks.\n",
    "\n",
    "Amazon Glue:\n",
    "Description: Amazon Glue is a fully managed extract, transform, and load (ETL) service that makes it easy to prepare and load data for analysis. \n",
    "It supports both batch and real-time processing.\n",
    "\n",
    "Google Cloud Dataprep:\n",
    "Description: Google Cloud Dataprep is a cloud-based data preparation service that allows users to visually explore, clean, and enrich data. \n",
    "It supports batch processing for preparing data at scale.\n",
    "\n",
    "IBM InfoSphere DataStage:\n",
    "Description: IBM InfoSphere DataStage is an ETL tool that facilitates the extraction, transformation, and loading of data. \n",
    "It supports batch processing for large-scale data integration tasks.\n",
    "\n",
    "Microsoft Azure Data Factory:\n",
    "Description: Azure Data Factory is a cloud-based data integration service by Microsoft Azure. \n",
    "It enables users to create, schedule, and manage data pipelines for batch processing and data movement.\n",
    "\n",
    "Informatica PowerCenter:\n",
    "Description: Informatica PowerCenter is an enterprise-grade ETL tool that supports batch processing for data integration and transformation. \n",
    "It provides a visual interface for designing and executing workflows.\n",
    "\n",
    "Oracle Data Integrator (ODI):\n",
    "Description: Oracle Data Integrator is a comprehensive data integration platform that includes ETL capabilities. \n",
    "It supports batch processing for loading, transforming, and managing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is snowflake? #\n",
    "\n",
    "Snowflake is a cloud-based data warehousing platform known for its scalable and managed solution. \n",
    "\n",
    "Its architecture separates storage and compute resources, offering flexibility and efficiency by allowing independent scaling. \n",
    "Snowflake supports diverse data workloads, enabling users to query structured and semi-structured data using SQL. \n",
    "\n",
    "With features like automatic optimization, built-in security, and a pay-as-you-go model, Snowflake simplifies data management and analytics in the cloud, making it a popular choice for organizations seeking a modern and efficient data warehousing solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is databricks? #\n",
    "\n",
    "Databricks is a cloud-based platform that simplifies big data analytics and AI. It combines Apache Spark with a collaborative environment for data science and machine learning. \n",
    "\n",
    "Databricks provides a unified workspace, allowing users to seamlessly collaborate on data processing, analytics, and machine learning tasks. \n",
    "Its cloud-native design ensures scalability and flexibility for handling large-scale data workloads.\n",
    "\n",
    "With Databricks, organizations can leverage Spark's processing power to analyze and process data efficiently. \n",
    "The platform supports various programming languages, interactive notebooks, and pre-built libraries for machine learning, making it accessible for data scientists and engineers. \n",
    "Databricks automates infrastructure management, offering a fully managed and optimized environment for deploying and scaling Spark-based applications. \n",
    "\n",
    "Overall, Databricks streamlines the data analytics and machine learning workflow, empowering teams to derive insights and build models collaboratively in a cloud environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Tableau? #\n",
    "\n",
    "Tableau is a leading data visualization and business intelligence platform that enables users to create interactive and insightful visualizations from their data. \n",
    "Known for its user-friendly interface, Tableau allows individuals and organizations to explore, analyze, and present data in a visually compelling way. \n",
    "Users can connect Tableau to various data sources, ranging from spreadsheets to large databases, to generate dynamic and interactive dashboards.\n",
    "\n",
    "One of Tableau's key strengths lies in its drag-and-drop functionality, which simplifies the creation of charts, graphs, and dashboards without requiring extensive coding or technical expertise. The platform supports real-time data connections, allowing users to work with live data for up-to-the-minute insights. Additionally, Tableau offers sharing and collaboration features, enabling teams to collaborate on visualizations and share their findings across the organization.\n",
    "\n",
    "Tableau's flexibility, ease of use, and ability to turn complex data into actionable insights have made it a popular choice for businesses of all sizes. \n",
    "Whether used for ad-hoc analysis, reporting, or creating data-driven presentations, Tableau empowers users to make data-driven decisions through compelling and interactive visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is an ERD (databases)? #\n",
    "\n",
    "An Entity-Relationship Diagram (ERD) is a visual representation used in database design to illustrate the relationships and structure of data in a relational database. \n",
    "Entities, represented as rectangles, correspond to tables, while attributes within entities denote fields. Relationships between entities are shown through lines, indicating how entities are connected, and diamond shapes depict the cardinality of the relationship. \n",
    "\n",
    "ERDs serve as a blueprint for database development, aiding in the understanding and design of the data model. \n",
    "They are crucial for ensuring that the database schema accurately reflects the intended structure and relationships of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Airflow? #\n",
    "\n",
    "Apache Airflow is an open-source platform designed for orchestrating complex workflows and data pipelines. \n",
    "Developed by the Apache Software Foundation, Airflow simplifies the process of creating, scheduling, and monitoring workflows, making it a popular choice for managing data workflows, ETL processes, and other automation tasks.\n",
    "\n",
    "Airflow uses Directed Acyclic Graphs (DAGs) to define and organize workflows. \n",
    "DAGs are representations of the workflow tasks and their dependencies, allowing users to specify the order in which tasks should be executed. Airflow supports a wide range of integrations, allowing users to connect to various data sources, databases, and cloud services.\n",
    "\n",
    "Key features of Apache Airflow include a dynamic scheduler, extensibility through plugins, a web-based user interface for monitoring and managing workflows, and the ability to define workflows as code. Its flexibility, scalability, and active community support have contributed to its widespread adoption for managing and automating data workflows in diverse industries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is MongoDB? #\n",
    "\n",
    "MongoDB is an open-source NoSQL (Not Only SQL) database system known for storing and retrieving unstructured or semi-structured data. \n",
    "It uses a flexible, document-oriented model, storing data in JSON-like documents called BSON. \n",
    "MongoDB is schema-less, allowing varied fields in the same collection, and it scales horizontally to handle large data volumes. \n",
    "With features like indexing and powerful querying, MongoDB is widely used for applications requiring flexibility and scalability in data storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is alteryx? #\n",
    "\n",
    "Alteryx is a data analytics platform that streamlines the process of preparing, blending, and analyzing data. \n",
    "It offers a user-friendly interface for designing data workflows, making it accessible to both data analysts and business professionals. \n",
    "Alteryx supports tasks such as data cleansing, transformation, and advanced analytics, providing a comprehensive solution for data preparation and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is talend? #\n",
    "\n",
    "Talend is a data integration and ETL (Extract, Transform, Load) platform that facilitates the movement and transformation of data between various systems. \n",
    "It provides a user-friendly interface for designing data integration workflows and supports connectivity to diverse data sources. \n",
    "Talend is known for its open-source nature and scalability, making it a popular choice for organizations seeking effective data integration solutions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
